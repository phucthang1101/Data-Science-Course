{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "edc8a8e7",
   "metadata": {},
   "source": [
    "# Deep Neural Network for MNIST Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "376e59a3",
   "metadata": {},
   "source": [
    "We'll apply all the knowledge from the lectures in this section to write a deep neural network. The problem we've chosen is referred to as the \"Hello World\" of deep learning because for most students it is the first deep learning algorithm they see.\n",
    "\n",
    "The dataset provides 70,000 images (28x28 pixels) of handwritten digits (1 digit per image).\n",
    "\n",
    "The goal is to write an algorithm that detects which digit is written. Since there are only 10 digits (0, 1, 2, 3, 4, 5, 6, 7, 8, 9), this is a classification problem with 10 classes.\n",
    "\n",
    "Our goal would be to build a neural network with 2 hidden layers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c615ba86",
   "metadata": {},
   "source": [
    "## Import relevant librabries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "38d0a1a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "import tensorflow_datasets as tfds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "e8b8d7a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.4.0\n"
     ]
    }
   ],
   "source": [
    "import h5py\n",
    "print(h5py.__version__);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b2c09b7",
   "metadata": {},
   "source": [
    "## Import data from tensorflow dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eeeed02",
   "metadata": {},
   "source": [
    "<b>tfsd.load(name, with_info, as_supervised)</b> loads a dataset from Tensorflow datasets\n",
    "\n",
    "-> <b>as_supervised = True</b>, loads the data in 2-tuple structure [input, target], alternatively, as_supervised=False, would return a dictionary\n",
    "\n",
    "\n",
    "-> <b>with_info = True</b>, will also provide us with a tuple containing information about the version, features, number of samples\n",
    "\n",
    "we will use this information a bit below and we will store it in mnist_info\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "67301c2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_dataset, mnist_info = tfds.load(name='mnist', with_info=True, as_supervised=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f5cd6e1",
   "metadata": {},
   "source": [
    "## Split into training and testing dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9eb718f",
   "metadata": {},
   "source": [
    "<b>tf.cast(x, dtype) casts (converts) a variable into a given data type</b>\n",
    "\n",
    "<b> Standardization: </b>Normally, we'd like to scale our data in some ways to make the result more numerically stable (e.g. inputs between 0 and 1).\n",
    "\n",
    "Each pixel contain a number from 0 -> 255, representing the 256 levels of shades of gray. Therefore, we divide each pixel by 255 to get desired result (between 0 and 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "3a9a352b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# once we have loaded the dataset, we can easily extract the training and testing dataset with the built references\n",
    "mnist_train, mnist_test = mnist_dataset['train'], mnist_dataset['test']\n",
    "\n",
    "# by default, TF has training and testing datasets, but no validation sets\n",
    "# thus we must split it on our own\n",
    "\n",
    "# we start by defining the number of validation samples as a % of the train samples\n",
    "# this is also where we make use of mnist_info (we don't have to count the observations)\n",
    "num_validation_samples = 0.1 * mnist_info.splits['train'].num_examples\n",
    "# let's cast this number to an integer, as a float may cause an error along the way\n",
    "num_validation_samples = tf.cast(num_validation_samples, tf.int64)\n",
    "\n",
    "# let's also store the number of test samples in a dedicated variable (instead of using the mnist_info one)\n",
    "num_test_samples = mnist_info.splits['test'].num_examples\n",
    "# once more, we'd prefer an integer (rather than the default float)\n",
    "num_test_samples = tf.cast(num_test_samples, tf.int64)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2343d170",
   "metadata": {},
   "source": [
    "## Scale the data (both train and test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "1d3d0e6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale(image, label):\n",
    "    image = tf.cast(image, tf.float32)\n",
    "    image /= 255.\n",
    "    return image, label\n",
    "\n",
    "# the method .map() allows us to apply a custom transformation to a given dataset\n",
    "# we have already decided that we will get the validation data from mnist_train, so \n",
    "scaled_train_and_validation_data = mnist_train.map(scale)\n",
    "\n",
    "\n",
    "test_data = mnist_test.map(scale)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3995735",
   "metadata": {},
   "source": [
    "## Shuffle the data randomly"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e796f52",
   "metadata": {},
   "source": [
    "<b>take(number):</b> extracting the number of samples from the train and validation datasets \n",
    "\n",
    "<b>skip(numer):</b> skipping the number of samples <b>(skip the validation data)</b> from the train and validation datasets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "a460d860",
   "metadata": {},
   "outputs": [],
   "source": [
    "BUFFER_SIZE = 10000\n",
    "# this BUFFER_SIZE parameter is here for cases when we're dealing with enormous datasets\n",
    "# then we can't shuffle the whole dataset in one go because we can't fit it all in memory\n",
    "# so instead TF only stores BUFFER_SIZE samples in memory at a time and shuffles them\n",
    "# if BUFFER_SIZE=1 => no shuffling will actually happen\n",
    "# if BUFFER_SIZE >= num samples => shuffling is uniform\n",
    "# BUFFER_SIZE in between - a computational optimization to approximate uniform shuffling\n",
    "\n",
    "shuffled_train_and_validation_data = scaled_train_and_validation_data.shuffle(BUFFER_SIZE)\n",
    "\n",
    "# once we have scaled and shuffled the data, we can proceed to actually extracting the train and validation\n",
    "# our validation data would be equal to 10% of the training set, which we've already calculated\n",
    "validation_data = shuffled_train_and_validation_data.take(num_validation_samples)\n",
    "train_data = shuffled_train_and_validation_data.skip(num_validation_samples)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b21bf41d",
   "metadata": {},
   "source": [
    "## Set batch size and prepare our data for batching"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "419ca085",
   "metadata": {},
   "source": [
    "batch_size = 1 = Stochastic gradient descent (SGD)\n",
    "\n",
    "batch_size = number of samples = single batch gradient descent \n",
    "\n",
    "1 < batch size < number of samples = mini-batch gradient descent "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11a66301",
   "metadata": {},
   "source": [
    "<b>dataset.batch(batch_size): </b>a method that combines the consecutive elements of a dataset into batches\n",
    "\n",
    "<b>iter(): </b>creates an object which can be iterated one element at a time (e.g. in a for loop or while loop)\n",
    "\n",
    "<b>next(): </b>loads the next element of an iterable object -> wrap <b>iter()</b> inside a <b>next()</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "75502d7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 100\n",
    "\n",
    "train_data = train_data.batch(BATCH_SIZE)\n",
    "\n",
    "validation_data = validation_data.batch(num_validation_samples)\n",
    "\n",
    "test_data = test_data.batch(num_test_samples)\n",
    "\n",
    "# takes next batch (it is the only batch)\n",
    "# because as_supervized=True, we've got a 2-tuple structure\n",
    "validation_inputs, validation_targets = next(iter(validation_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49b8aa43",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2bb7834",
   "metadata": {},
   "source": [
    "### Outline the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "537a1a1d",
   "metadata": {},
   "source": [
    "<b>tf.keras.Sequential()</b>: function that is laying down the model (used to 'stack layers)\n",
    "\n",
    "<b>tf.keras.layers.Flatten(original shape)</b>: transforms (flattens) a tensor into a vector  \n",
    "\n",
    "<b>tf.keras.layers.Dense(units,activation)</b> implements the operation: <b>output = activation(dot(input, kernel) + bias)</b> where <b>activation</b> is the <b>activation function</b> passed as the activation argument, <b>kernel</b> is a weights matrix created by the layer, and <b>bias</b> is a bias vector created by the layer, and <b>units</b> is positive integer, dimensionality of the output space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "2c01dc04",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 784\n",
    "output_size = 10\n",
    "# Use same hidden layer size for both hidden layers. Not a necessity.\n",
    "hidden_layer_size = 50\n",
    "    \n",
    "# define how the model will look like\n",
    "model = tf.keras.Sequential([\n",
    "    \n",
    "    # the first layer (the input layer)\n",
    "    # each observation is 28x28x1 pixels, therefore it is a tensor of rank 3\n",
    "    # since we don't know CNNs yet, we don't know how to feed such input into our net, so we must flatten the images\n",
    "    # there is a convenient method 'Flatten' that simply takes our 28x28x1 tensor and orders it into a (None,) \n",
    "    # or (28x28x1,) = (784,) vector\n",
    "    # this allows us to actually create a feed forward neural network\n",
    "    tf.keras.layers.Flatten(input_shape=(28, 28, 1)), # input layer\n",
    "    \n",
    "    # tf.keras.layers.Dense is basically implementing: output = activation(dot(input, weight) + bias)\n",
    "    # it takes several arguments, but the most important ones for us are the hidden_layer_size and the activation function\n",
    "    tf.keras.layers.Dense(hidden_layer_size, activation='relu'), # 1st hidden layer\n",
    "    tf.keras.layers.Dense(hidden_layer_size, activation='relu'), # 2nd hidden layer\n",
    "    \n",
    "    # the final layer is no different, we just make sure to activate it with softmax\n",
    "    tf.keras.layers.Dense(output_size, activation='softmax') # output layer\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "705eaa72",
   "metadata": {},
   "source": [
    "## Choose the optimizer and the loss function\n",
    "\n",
    "1. Data (done)\n",
    "\n",
    "2. Model (done)\n",
    "\n",
    "3. Objective function -> current\n",
    "\n",
    "4. Optimization algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "321b8c99",
   "metadata": {},
   "source": [
    "There are 3 built-in variations of across entropy-loss\n",
    "\n",
    "<b>binary_crossentropy</b>: refers to the case where we've got <b>binary encoding</b>\n",
    "\n",
    "<b>categorical_crossentropy</b>: expects that you've one-hot encoded the targets\n",
    "\n",
    "<b>sparse_categorical_crossentropy</b>: applies one-hot encoding \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d29db85",
   "metadata": {},
   "source": [
    "<b>model.compile(optimizer, loss, metrics)</b> configures the model for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "9936870b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c88bfe4",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea277918",
   "metadata": {},
   "source": [
    "https://www.tensorflow.org/api_docs/python/tf/keras/Model#fit\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53fb8770",
   "metadata": {},
   "source": [
    "## What happens inside an epoch?\n",
    "\n",
    "1. At the beginning of each epoch, the training loss will be set to 0\n",
    "\n",
    "2. the algorithm will iterate over a preset number of batches, all from train_data\n",
    "\n",
    "3. The weights and biases will be updated as many time as there are batches\n",
    "\n",
    "4. We will get a value for a loss function, indicating how the training is going\n",
    "\n",
    "5. We will also see a training accuracy \n",
    "\n",
    "6. At the end of epoch, the algorithm will foward propagate the whole validation set.\n",
    "\n",
    "*When we reach the maximum number of epochs the training will be over"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "4b45f43b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "540/540 - 3s - loss: 0.4190 - accuracy: 0.8807 - val_loss: 0.2134 - val_accuracy: 0.9412\n",
      "Epoch 2/5\n",
      "540/540 - 3s - loss: 0.1854 - accuracy: 0.9460 - val_loss: 0.1550 - val_accuracy: 0.9562\n",
      "Epoch 3/5\n",
      "540/540 - 3s - loss: 0.1394 - accuracy: 0.9588 - val_loss: 0.1258 - val_accuracy: 0.9653\n",
      "Epoch 4/5\n",
      "540/540 - 3s - loss: 0.1137 - accuracy: 0.9666 - val_loss: 0.1096 - val_accuracy: 0.9673\n",
      "Epoch 5/5\n",
      "540/540 - 3s - loss: 0.0936 - accuracy: 0.9719 - val_loss: 0.0985 - val_accuracy: 0.9712\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1b703d8d880>"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NUM_EPOCHS = 5\n",
    "\n",
    "model.fit(train_data, epochs=NUM_EPOCHS, validation_data=(validation_inputs, validation_targets), verbose =2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9240a2f8",
   "metadata": {},
   "source": [
    "## Test the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "114bbe2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 2ms/step - loss: 0.1023 - accuracy: 0.9684\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_accuracy = model.evaluate(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "41a634c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.10. Test accuracy: 96.84%\n"
     ]
    }
   ],
   "source": [
    "# We can apply some nice formatting if we want to\n",
    "print('Test loss: {0:.2f}. Test accuracy: {1:.2f}%'.format(test_loss, test_accuracy*100.))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a06ca717",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:root] *",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
